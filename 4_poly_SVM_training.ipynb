{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pKu16A3RDYzR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#import random\n",
    "#import optuna\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "\n",
    "print(sklearn.__version__)\n",
    "import random\n",
    "#import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZanWokw4D_m6",
    "outputId": "556ea353-03a1-448c-d383-04a4cb114bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      q1     q2      r1      r2        x1        x2  ismisc\n",
      "0  1.432  2.588  1.4311  2.5755  0.312852  0.132230       1\n",
      "1  1.432  2.588  1.4311  2.5755  0.362349  0.564846       1\n",
      "2  1.432  2.588  1.4311  2.5755  0.507916  0.181390       1\n",
      "3  1.432  2.588  1.4311  2.5755  0.044826  0.656056       1\n",
      "4  1.432  2.588  1.4311  2.5755  0.255614  0.554332       1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/kainath56/researchproject/raw/main/dataset_binarymixture_final.xlsx\"\n",
    "raw = pd.read_excel(url)\n",
    "df = raw.copy()\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['Unnamed: 0', 'Solvent 1', 'Solvent 2', 'T'])\n",
    "df2 =  df.copy()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#converting each column into variables\n",
    "q1 = df2.pop('q1')\n",
    "q2 = df2.pop('q2')\n",
    "r1 = df2.pop('r1')\n",
    "r2 = df2.pop('r2')\n",
    "x1 = df2.pop('x1')\n",
    "x2 = df2.pop('x2')\n",
    "ismisc = df2.pop('ismisc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KKMdYzZNVK_",
    "outputId": "2d62aaf1-a032-4e17-a047-33d2a4315bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18432, 6)\n",
      "(18432, 1)\n"
     ]
    }
   ],
   "source": [
    "features = np.array((q1, q2,r1,r2, x1, x2)) #this gives a (dimension, samples) dataset\n",
    "features=features.transpose() #this gives (samples,dimension)\n",
    "print(features.shape)\n",
    "\n",
    "label = np.array(ismisc) \n",
    "label=label.reshape(-1,1) \n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L3DkucUA9qLC"
   },
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "scalar.fit(features);\n",
    "x = scalar.transform(features)\n",
    "\n",
    "y=label #no need to do scaling of y because it is a binary variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gq5x6Jj1w41B",
    "outputId": "1f1158bb-9abd-45f6-936e-ff778d407a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12902, 6) (5530, 6) (12902, 1) (5530, 1)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest= train_test_split(x,y, test_size=0.3, shuffle=True, random_state=12, stratify=y) #turning shuffle on because currently the data is ordered by ismisc column\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
    "\n",
    "#Xtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=2765/15667, shuffle=True, random_state=12, stratify=ytrain)\n",
    "#print(Xtrain.shape, Xtest.shape, Xval.shape,ytrain.shape, ytest.shape, yval.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i61uFTftX6f4"
   },
   "source": [
    "## SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeleval(ytestt, ypred):\n",
    "  accuracy = accuracy_score(ytestt, ypred)\n",
    "  mcc = matthews_corrcoef(ytestt, SVCypred.ravel())\n",
    "  print(\"Prediction accuracy:\", accuracy)\n",
    "  print(\"Prediction mcc:\", mcc)\n",
    "  print(\"Classification report:\\n\", classification_report(ytestt,ypred))\n",
    "  cfm = confusion_matrix(ytestt,ypred)\n",
    "  disp = ConfusionMatrixDisplay(cfm)\n",
    "  disp.plot()\n",
    "  plt.gca().invert_yaxis()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n",
      "151\n",
      "161\n",
      "171\n",
      "181\n",
      "191\n",
      "201\n",
      "211\n",
      "221\n",
      "231\n",
      "241\n",
      "251\n",
      "261\n",
      "271\n",
      "281\n",
      "291\n",
      "301\n",
      "311\n",
      "321\n",
      "331\n",
      "341\n",
      "351\n",
      "361\n",
      "371\n",
      "381\n",
      "391\n",
      "401\n",
      "411\n",
      "421\n",
      "431\n",
      "441\n",
      "451\n",
      "461\n",
      "471\n",
      "481\n",
      "491\n",
      "501\n",
      "511\n",
      "521\n",
      "531\n",
      "541\n",
      "551\n",
      "561\n",
      "571\n",
      "581\n",
      "591\n",
      "601\n",
      "611\n",
      "621\n",
      "631\n",
      "641\n",
      "651\n",
      "661\n",
      "671\n",
      "681\n",
      "691\n",
      "701\n",
      "711\n",
      "721\n",
      "731\n",
      "741\n",
      "751\n",
      "761\n",
      "771\n",
      "781\n",
      "791\n",
      "801\n",
      "811\n",
      "821\n",
      "831\n",
      "841\n",
      "851\n",
      "861\n",
      "871\n",
      "881\n",
      "891\n",
      "901\n",
      "911\n",
      "921\n",
      "931\n",
      "941\n",
      "951\n",
      "961\n",
      "971\n",
      "981\n",
      "991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nbin_c_4 = []\\nbin_sv_4=[]\\nbin_mcc_train_4=[]\\nbin_mcc_test_4=[]\\nfor i in range(1,100,10):\\n    print(i)\\n    params = {'C': i*0.01, 'degree': }\\n    SVCmodel = SVC(kernel='poly', C=params['C'], degree=params['degree'], random_state=12, class_weight='balanced', probability=True)\\n    SVCmodel.fit(Xtrain, ytrain.ravel())\\n    SVCytrain = SVCmodel.predict(Xtrain)\\n    SVCytest = SVCmodel.predict(Xtest)\\n    bin_mcc_train_4.append(matthews_corrcoef(ytrain.ravel(), SVCytrain))\\n    bin_mcc_test_4.append(matthews_corrcoef(ytest.ravel(), SVCytest))\\n    bin_c_4.append(i*0.01)\\n    bin_sv_4.append(len(SVCmodel.support_vectors_))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_c_2 = []\n",
    "bin_sv_2=[]\n",
    "bin_mcc_train_2=[]\n",
    "bin_mcc_test_2=[]\n",
    "for i in range(1,1000,10):\n",
    "    print(i)\n",
    "    params = {'C': i*0.01, 'degree': 8}\n",
    "    SVCmodel = SVC(kernel='poly', C=params['C'], degree=params['degree'], random_state=12, class_weight='balanced', probability=True)\n",
    "    SVCmodel.fit(Xtrain, ytrain.ravel())\n",
    "    SVCytrain = SVCmodel.predict(Xtrain)\n",
    "    SVCytest = SVCmodel.predict(Xtest)\n",
    "    bin_mcc_train_2.append(matthews_corrcoef(ytrain.ravel(), SVCytrain))\n",
    "    bin_mcc_test_2.append(matthews_corrcoef(ytest.ravel(), SVCytest))\n",
    "    bin_c_2.append(i*0.01)\n",
    "    bin_sv_2.append(len(SVCmodel.support_vectors_))\n",
    "    \n",
    "\"\"\"\n",
    "bin_c_4 = []\n",
    "bin_sv_4=[]\n",
    "bin_mcc_train_4=[]\n",
    "bin_mcc_test_4=[]\n",
    "for i in range(1,100,10):\n",
    "    print(i)\n",
    "    params = {'C': i*0.01, 'degree': }\n",
    "    SVCmodel = SVC(kernel='poly', C=params['C'], degree=params['degree'], random_state=12, class_weight='balanced', probability=True)\n",
    "    SVCmodel.fit(Xtrain, ytrain.ravel())\n",
    "    SVCytrain = SVCmodel.predict(Xtrain)\n",
    "    SVCytest = SVCmodel.predict(Xtest)\n",
    "    bin_mcc_train_4.append(matthews_corrcoef(ytrain.ravel(), SVCytrain))\n",
    "    bin_mcc_test_4.append(matthews_corrcoef(ytest.ravel(), SVCytest))\n",
    "    bin_c_4.append(i*0.01)\n",
    "    bin_sv_4.append(len(SVCmodel.support_vectors_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport csv\\ndata = zip(bin_c_2,bin_sv_2,bin_mcc_train_2,bin_mcc_test_2,\\n           bin_c_4,bin_sv_4,bin_mcc_train_4,bin_mcc_test_4)\\nwith open('graphs_binary2.csv','w', newline='') as csvfile:\\n    csvwriter=csv.writer(csvfile)\\n    csvwriter.writerow(['c','sv','mcc1', 'mcc2', 'c','sv','mcc1', 'mcc2'])\\n    csvwriter.writerows(data)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(bin_c_2,bin_mcc_test_2)\n",
    "    \n",
    "\"\"\"\n",
    "import csv\n",
    "data = zip(bin_c_2,bin_sv_2,bin_mcc_train_2,bin_mcc_test_2,\n",
    "           bin_c_4,bin_sv_4,bin_mcc_train_4,bin_mcc_test_4)\n",
    "with open('graphs_binary2.csv','w', newline='') as csvfile:\n",
    "    csvwriter=csv.writer(csvfile)\n",
    "    csvwriter.writerow(['c','sv','mcc1', 'mcc2', 'c','sv','mcc1', 'mcc2'])\n",
    "    csvwriter.writerows(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "U2hbDd4HsiDA",
    "outputId": "ad39f8d1-ada5-40ca-d6a5-8d2d0732b025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9596442283185366\n",
      "0.9700270547560427\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "params = {'C2': 15, 'deg2': 2, 'C4':5, 'deg4':4}\n",
    "\n",
    "###################fitting the model##########################################\n",
    "SVCmodel = SVC(kernel='rbf', C=params['C4'], degree=params['deg4'], random_state=12, class_weight='balanced', probability=True)\n",
    "SVCmodel.fit(Xtrain, ytrain.ravel())\n",
    "SVCytrain = SVCmodel.predict(Xtrain)\n",
    "SVCytest = SVCmodel.predict(Xtest)\n",
    "SVCy=SVCmodel.predict(x)\n",
    "#SVCyval = SVCmodel.predict(Xval)\n",
    "print(matthews_corrcoef(ytrain.ravel(), SVCytrain))\n",
    "print(matthews_corrcoef(ytest.ravel(), SVCytest))\n",
    "\n",
    "print(len(SVCmodel.support_vectors_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#stratified kfold\n",
    "scorer = make_scorer(matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=12)\n",
    "accuracy = []\n",
    "for train_index, test_index in skf.split(Xtest,ytest.ravel()):\n",
    "    x1_train, x1_test = Xtest[train_index], Xtest[test_index]\n",
    "    y1_train, y1_test = ytest[train_index], ytest[test_index]\n",
    "    SVCmodel2 = SVC(kernel='poly', C=params['C'], degree=params['degree'], random_state=12, class_weight='balanced', probability=True)\n",
    "    SVCmodel2.fit(x1_train,y1_train.ravel())\n",
    "    prediction = SVCmodel2.predict(x1_test)\n",
    "    score = matthews_corrcoef(prediction, y1_test.ravel())\n",
    "    print(\"Score\", score)\n",
    "    accuracy.append(score)\n",
    "\n",
    "print(\"Average score,\", sum(accuracy)/len(accuracy))\n",
    "print(\"Max score\", max(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer to gams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n"
     ]
    }
   ],
   "source": [
    "%reload_ext gams_magic\n",
    "%gams_cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run DataTransform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 6)\n",
      "q1    2.581667\n",
      "q2    2.601889\n",
      "r1    2.786711\n",
      "r2    2.905844\n",
      "x1    0.333361\n",
      "x2    0.333258\n",
      "Name: mean, dtype: float64\n",
      "q1    0.767713\n",
      "q2    0.783279\n",
      "r1    0.948082\n",
      "r2    1.208076\n",
      "x1    0.180004\n",
      "x2    0.179409\n",
      "Name: std, dtype: float64\n",
      "mean aka offset: [2.58166667 2.60188889 2.78671111 2.90584444 0.33336146 0.33325809]\n",
      "gain aka 1/std : [1.30257016 1.27668481 1.05476087 0.82776256 5.55544541 5.57387066]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x_sv = SVCmodel.support_vectors_\n",
    "y_sv = SVCmodel.predict(x_sv)*2-1  #map the y to -1,1\n",
    "v_sv = SVCmodel.dual_coef_.ravel()\n",
    "bias= float(SVCmodel.intercept_)\n",
    "print(x_sv.shape)\n",
    "\n",
    "\n",
    "df3 = df.copy()\n",
    "df3 = df3.drop(columns=['ismisc'])\n",
    "a=df3.describe()\n",
    "mean= a.loc['mean']\n",
    "std=(a.loc['std'])\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "input_offset = np.array(mean)\n",
    "input_gain = np.array(1/std)\n",
    "\n",
    "print(\"mean aka offset:\", input_offset)\n",
    "print(\"gain aka 1/std :\", input_gain )\n",
    "\n",
    "d = params['deg4']\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#checking prediction of my decision function against python's prediction \n",
    "\n",
    "#SVCmodel = SVC(kernel='poly', C=params['C'], degree=params['degree'], random_state=12, class_weight='balanced', probability=False)\n",
    "#SVCmodel.fit(Xtrain, ytrain.ravel())\n",
    "\n",
    "\n",
    "v_sv = SVCmodel.dual_coef_.flatten()\n",
    "x_sv = SVCmodel.support_vectors_\n",
    "bias = SVCmodel.intercept_[0]\n",
    "\n",
    "#Checking custom decision function against scikit-learn's predictions\n",
    "pred = []\n",
    "for i in range(len(Xtest)):\n",
    "    xt = Xtest[i, :].reshape(1, -1)\n",
    "    yt = SVCmodel.predict(xt) * 2 - 1\n",
    "\n",
    "    # Predicting using weights and kernel function\n",
    "    kernel = (x_sv.dot(xt.T)+1) ** 2\n",
    "    prediction = np.sign(np.sum(v_sv * kernel) + bias)\n",
    "\n",
    "    # Prediction of weights == prediction by scikit-learn?\n",
    "    pred.append((prediction == yt).tolist())\n",
    "\n",
    "pred = np.ravel(pred).tolist()\n",
    "a = pred.count(True)\n",
    "b = pred.count(False)\n",
    "print(a / (a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gams SETS f  /1*6/;\n",
    "%gams SETS sv      'support vectors samples' /1*264/;\n",
    "%gams_pull f sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gams\n",
    "\n",
    "TABLE x_sv(sv,f);\n",
    "\n",
    "Parameters\n",
    "v_sv(sv)\n",
    "input_offset(f)\n",
    "input_gain(f)\n",
    "\n",
    ";\n",
    "SCALAR\n",
    "bias\n",
    "d\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     28 PARAMETER x_sv  2-dim Matrix\n",
      "\n",
      "              1           2           3           4           5           6\n",
      "\n",
      "1         0.706      -1.534       0.488      -1.644      -0.661       2.296\n",
      "2        -0.224      -1.534       0.088      -1.644       2.874      -1.704\n",
      "3         0.696      -1.534       0.730      -1.644       0.544      -1.341\n",
      "4        -0.224      -1.534       0.088      -1.644      -1.504       2.881\n",
      "5         0.696      -1.534       0.730      -1.644       1.449      -1.157\n",
      "6         0.706      -1.534       0.488      -1.644      -1.240       0.388\n",
      "7         1.785      -1.534       1.908      -1.644       0.235      -1.318\n",
      "8        -0.224      -1.534       0.088      -1.644       0.502       0.600\n",
      "9         0.706      -1.534       0.488      -1.644      -1.024       0.956\n",
      "10        0.696      -1.534       0.730      -1.644       1.350      -1.120\n",
      "11        1.785      -1.534       1.908      -1.644       0.722      -1.181\n",
      "12        0.706      -1.534       0.488      -1.644      -1.219       0.535\n",
      "13       -0.224      -1.534       0.088      -1.644      -1.768       2.042\n",
      "14        0.706      -1.534       0.488      -1.644      -1.317       0.232\n",
      "15        0.706      -1.534       0.488      -1.644      -1.121       0.904\n",
      "16       -0.224      -1.534       0.088      -1.644      -0.021       0.047\n",
      "17        0.706      -1.534       0.488      -1.644      -1.383      -0.038\n",
      "18       -0.224      -1.534       0.088      -1.644       0.054       0.168\n",
      "19        0.696      -1.534       0.730      -1.644       1.071      -1.192\n",
      "20        0.696      -1.534       0.730      -1.644       1.301      -1.181\n",
      "21        0.696      -1.534       0.730      -1.644       0.641      -1.338\n",
      "22       -0.224      -1.534       0.088      -1.644       0.063       0.142\n",
      "23       -0.224      -1.534       0.088      -1.644       0.932       0.811\n",
      "24        0.696      -1.534       0.730      -1.644      -0.470      -1.564\n",
      "25        0.706      -1.534       0.488      -1.644      -1.206       0.752\n",
      "26        0.706      -1.534       0.488      -1.644      -1.097       1.294\n",
      "27       -0.224      -1.534       0.088      -1.644      -1.832      -1.344\n",
      "28        1.785      -1.534       1.908      -1.644      -1.504       2.881\n",
      "29        0.696      -1.534       0.730      -1.644       0.877       0.922\n",
      "30       -0.224      -1.534       0.088      -1.644       0.234       0.371\n",
      "31        0.706      -1.534       0.488      -1.644      -0.998       1.207\n",
      "32       -0.224      -1.534       0.088      -1.644      -0.251      -0.024\n",
      "33        0.696      -1.534       0.730      -1.644       0.099      -1.452\n",
      "34        1.785      -1.534       1.908      -1.644      -1.253      -1.684\n",
      "35        0.706      -1.534       0.488      -1.644      -1.263       0.157\n",
      "36        1.785      -1.534       1.908      -1.644       2.284      -0.743\n",
      "37        0.706      -1.534       0.488      -1.644      -1.685      -0.774\n",
      "38       -0.224      -1.534       0.088      -1.644      -1.253      -1.684\n",
      "39        0.706      -1.534       0.488      -1.644      -0.751       2.180\n",
      "40        0.706      -1.534       0.488      -1.644      -0.840       2.035\n",
      "41        0.706      -1.534       0.488      -1.644      -0.209       0.609\n",
      "42        0.696      -1.534       0.730      -1.644       1.329      -1.474\n",
      "43        0.706      -1.534       0.488      -1.644      -0.541      -0.138\n",
      "44        0.706      -1.534       0.488      -1.644      -1.235       0.044\n",
      "45        0.706      -1.534       0.488      -1.644      -0.509      -0.067\n",
      "46        0.706      -1.534       0.488      -1.644       0.122       1.609\n",
      "47        0.706      -1.534       0.488      -1.644      -1.578       0.624\n",
      "48        0.008       1.724      -0.223       1.399       0.388      -1.763\n",
      "49        0.706      -1.534       0.488      -1.644      -1.263      -0.144\n",
      "50        0.706      -1.534       0.488      -1.644      -1.786       1.965\n",
      "51        0.706      -1.534       0.488      -1.644      -0.932       0.427\n",
      "52        0.696      -1.534       0.730      -1.644       2.588      -1.111\n",
      "53        0.696      -1.534       0.730      -1.644       1.663      -1.487\n",
      "54        1.785      -1.534       1.908      -1.644      -0.470      -1.564\n",
      "55        0.706      -1.534       0.488      -1.644      -0.712       0.882\n",
      "56        1.785      -1.534       1.908      -1.644      -0.258      -1.616\n",
      "57        0.706      -1.534       0.488      -1.644      -0.666       0.443\n",
      "58        0.706      -1.534       0.488      -1.644      -1.800       0.136\n",
      "59        0.706      -1.534       0.488      -1.644      -1.750       1.102\n",
      "60       -0.320      -1.534      -0.225      -1.644      -0.718       1.194\n",
      "61        0.706      -1.534       0.488      -1.644      -1.504       2.881\n",
      "62        0.706      -1.534       0.488      -1.644      -0.357       1.514\n",
      "63        0.706      -1.534       0.488      -1.644      -0.973      -0.523\n",
      "64        0.706      -1.534       0.488      -1.644      -0.370       0.140\n",
      "65        0.706      -1.534       0.488      -1.644      -0.357       0.932\n",
      "66       -1.498       1.724      -1.430       1.399       2.036      -0.272\n",
      "67        0.706      -1.534       0.488      -1.644      -0.402       0.056\n",
      "68       -0.320      -1.534      -0.225      -1.644       1.314      -0.816\n",
      "69        0.706      -1.534       0.488      -1.644      -1.044      -0.454\n",
      "70        0.706      -1.534       0.488      -1.644      -0.834       0.843\n",
      "71        0.706      -1.534       0.488      -1.644      -0.328      -0.236\n",
      "72        0.706      -1.534       0.488      -1.644      -0.218       0.139\n",
      "73        0.706      -1.534       0.488      -1.644      -1.551      -1.176\n",
      "74        0.706      -1.534       0.488      -1.644      -1.358      -0.475\n",
      "75        0.706      -1.534       0.488      -1.644      -0.983       0.730\n",
      "76        0.706      -1.534       0.488      -1.644      -1.681       1.295\n",
      "77        0.706      -1.534       0.488      -1.644      -0.571       1.980\n",
      "78        0.706      -1.534       0.488      -1.644      -0.642       1.336\n",
      "79       -1.498       1.724      -1.430       1.399      -1.685      -0.774\n",
      "80        0.008       1.724      -0.223       1.399       1.585       0.113\n",
      "81        0.706      -1.534       0.488      -1.644      -0.917       1.016\n",
      "82       -1.498       1.724      -1.430       1.399       2.000      -0.350\n",
      "83        1.785      -1.534       1.908      -1.644       0.802      -1.362\n",
      "84        0.706      -1.534       0.488      -1.644      -0.727       1.027\n",
      "85        0.706      -1.534       0.488      -1.644      -0.848       0.303\n",
      "86        0.706      -1.534       0.488      -1.644      -0.295       0.449\n",
      "87        0.008       1.724      -0.223       1.399      -0.663       0.611\n",
      "88        0.696      -1.534       0.730      -1.644       2.206      -1.324\n",
      "89        0.706      -1.534       0.488      -1.644      -1.289      -0.811\n",
      "90        0.706      -1.534       0.488      -1.644      -0.339      -0.263\n",
      "91        0.706      -1.534       0.488      -1.644      -0.432       1.232\n",
      "92        0.706      -1.534       0.488      -1.644      -1.045       0.400\n",
      "93        0.706      -1.534       0.488      -1.644      -1.518       1.958\n",
      "94        1.785      -1.534       1.908      -1.644      -0.305      -1.589\n",
      "95        0.696      -1.534       0.730      -1.644       0.925      -1.654\n",
      "96        0.706      -1.534       0.488      -1.644      -0.663       0.611\n",
      "97       -0.320      -1.534      -0.225      -1.644      -1.434       0.276\n",
      "98        0.706      -1.534       0.488      -1.644      -0.635       0.817\n",
      "99        0.706      -1.534       0.488      -1.644      -1.661       1.051\n",
      "100       0.008       1.724      -0.223       1.399       1.710      -0.019\n",
      "101       0.706      -1.534       0.488      -1.644      -0.755       0.480\n",
      "102      -0.320      -1.534      -0.225      -1.644       1.806      -0.744\n",
      "103       0.008       1.724      -0.223       1.399      -1.216       0.765\n",
      "104       0.706      -1.534       0.488      -1.644      -1.505      -1.193\n",
      "105       1.785      -1.534       1.908      -1.644      -0.555      -1.729\n",
      "106       0.706      -1.534       0.488      -1.644      -1.697       0.004\n",
      "107       0.706      -1.534       0.488      -1.644      -0.316       1.655\n",
      "108       1.785      -1.534       1.908      -1.644       1.301      -1.181\n",
      "109       0.008       1.724      -0.223       1.399      -1.206       0.752\n",
      "110       0.706      -1.534       0.488      -1.644      -1.233       2.950\n",
      "111      -0.320      -1.534      -0.225      -1.644       1.373      -0.925\n",
      "112       0.008       1.724      -0.223       1.399       0.676      -1.561\n",
      "113       0.008       1.724      -0.223       1.399       0.708      -1.620\n",
      "114       0.706      -1.534       0.488      -1.644      -0.755      -0.712\n",
      "115       0.706      -1.534       0.488      -1.644      -0.015       1.578\n",
      "116       0.008       1.724      -0.223       1.399       0.308      -1.479\n",
      "117       0.008       1.724      -0.223       1.399      -0.834       0.843\n",
      "118       0.696      -1.534       0.730      -1.644       2.594      -1.029\n",
      "119       0.008       1.724      -0.223       1.399      -0.586       0.841\n",
      "120       0.706      -1.534       0.488      -1.644      -0.765       0.062\n",
      "121       1.785      -1.534       1.908      -1.644       1.071      -1.192\n",
      "122      -0.320      -1.534      -0.225      -1.644      -0.814       1.228\n",
      "123       0.696      -1.534       0.730      -1.644       2.074      -1.401\n",
      "124       0.696      -1.534       0.730      -1.644       2.004      -1.452\n",
      "125      -0.320      -1.534      -0.225      -1.644      -0.650       1.438\n",
      "126       0.696      -1.534       0.730      -1.644       0.226      -1.477\n",
      "127       0.706      -1.534       0.488      -1.644      -0.790       1.366\n",
      "128       0.706      -1.534       0.488      -1.644      -1.580       0.788\n",
      "129       0.706      -1.534       0.488      -1.644      -1.075      -1.225\n",
      "130       0.696      -1.534       0.730      -1.644       1.051      -1.709\n",
      "131       0.696      -1.534       0.730      -1.644       1.281      -1.261\n",
      "132       0.706      -1.534       0.488      -1.644      -1.290      -0.338\n",
      "133       0.696      -1.534       0.730      -1.644      -0.258      -1.616\n",
      "134       1.785      -1.534       1.908      -1.644       0.124      -1.371\n",
      "135       0.706      -1.534       0.488      -1.644      -1.832      -1.344\n",
      "136       0.696      -1.534       0.730      -1.644       1.266      -1.646\n",
      "137       0.706      -1.534       0.488      -1.644      -0.398       0.225\n",
      "138       0.706      -1.534       0.488      -1.644      -1.100      -0.114\n",
      "139       0.706      -1.534       0.488      -1.644      -1.766      -0.353\n",
      "140       0.706      -1.534       0.488      -1.644      -0.737      -0.019\n",
      "141       0.706      -1.534       0.488      -1.644      -0.676       1.325\n",
      "142      -0.320      -1.534      -0.225      -1.644       0.007       1.722\n",
      "143       0.706      -1.534       0.488      -1.644      -0.281       0.522\n",
      "144      -0.320      -1.534      -0.225      -1.644      -0.676       1.325\n",
      "145      -0.320      -1.534      -0.225      -1.644      -0.571       1.980\n",
      "146      -0.320      -1.534      -0.225      -1.644       2.306      -0.699\n",
      "147       0.696      -1.534       0.730      -1.644      -1.840       3.692\n",
      "148       0.706      -1.534       0.488      -1.644      -0.896      -0.974\n",
      "149       0.706      -1.534       0.488      -1.644      -0.766       0.222\n",
      "150       0.706      -1.534       0.488      -1.644      -1.572       1.323\n",
      "151       0.696      -1.534       0.730      -1.644      -1.838       0.344\n",
      "152       0.706      -1.534       0.488      -1.644      -0.437       0.398\n",
      "153      -0.320      -1.534      -0.225      -1.644      -0.573       1.568\n",
      "154       0.696      -1.534       0.730      -1.644       0.879      -1.364\n",
      "155       0.706      -1.534       0.488      -1.644      -0.430       0.371\n",
      "156       0.008       1.724      -0.223       1.399      -0.410       1.079\n",
      "157       0.706      -1.534       0.488      -1.644      -0.747       0.257\n",
      "158       0.706      -1.534       0.488      -1.644      -0.505       0.228\n",
      "159       0.696      -1.534       0.730      -1.644       0.688      -1.380\n",
      "160       0.706      -1.534       0.488      -1.644      -0.650       1.438\n",
      "161      -0.320      -1.534      -0.225      -1.644       1.350      -1.120\n",
      "162       0.706      -1.534       0.488      -1.644      -0.270       1.219\n",
      "163       0.696      -1.534       0.730      -1.644       1.973      -1.265\n",
      "164       0.706      -1.534       0.488      -1.644      -0.413      -0.518\n",
      "165       0.008       1.724      -0.223       1.399      -0.993       0.554\n",
      "166       0.706      -1.534       0.488      -1.644      -0.431      -0.260\n",
      "167       0.008       1.724      -0.223       1.399      -0.604       0.862\n",
      "168       0.706      -1.534       0.488      -1.644      -1.768       2.042\n",
      "169      -0.320      -1.534      -0.225      -1.644       2.004      -0.631\n",
      "170       0.706      -1.534       0.488      -1.644      -0.194       0.989\n",
      "171       0.696      -1.534       0.730      -1.644       0.272      -1.681\n",
      "172       0.706      -1.534       0.488      -1.644      -0.371       1.320\n",
      "173       0.706      -1.534       0.488      -1.644      -0.604       0.862\n",
      "174       0.706      -1.534       0.488      -1.644      -0.225       0.304\n",
      "175       0.706      -1.534       0.488      -1.644      -0.972      -0.015\n",
      "176       0.706      -1.534       0.488      -1.644      -1.629       2.195\n",
      "177      -0.320      -1.534      -0.225      -1.644      -0.357       1.514\n",
      "178       0.706      -1.534       0.488      -1.644      -0.745      -0.170\n",
      "179       0.706      -1.534       0.488      -1.644      -0.737       0.448\n",
      "180       0.706      -1.534       0.488      -1.644      -0.391       0.788\n",
      "181       0.706      -1.534       0.488      -1.644      -0.446      -0.760\n",
      "182       0.706      -1.534       0.488      -1.644      -0.574      -0.314\n",
      "183       1.785      -1.534       1.908      -1.644       0.879      -1.364\n",
      "184      -0.320      -1.534      -0.225      -1.644       2.117      -0.787\n",
      "185       0.706      -1.534       0.488      -1.644      -0.185       0.429\n",
      "186       0.696      -1.534       0.730      -1.644       1.475      -1.502\n",
      "187       0.696      -1.534       0.730      -1.644       0.802      -1.362\n",
      "188       0.706      -1.534       0.488      -1.644      -0.684       0.139\n",
      "189       1.785      -1.534       1.908      -1.644       1.350      -1.120\n",
      "190       0.696      -1.534       0.730      -1.644       2.009      -1.250\n",
      "191       0.696      -1.534       0.730      -1.644      -0.305      -1.589\n",
      "192      -1.498       0.467      -1.430       0.842      -1.832      -1.344\n",
      "193       1.785      -1.534       1.908      -1.644       1.047      -1.153\n",
      "194       0.706      -1.534       0.488      -1.644      -0.305       0.867\n",
      "195       0.696      -1.534       0.730      -1.644       1.320      -1.771\n",
      "196       0.706      -1.534       0.488      -1.644      -0.648       1.319\n",
      "197      -0.320      -1.534      -0.225      -1.644      -1.418       0.291\n",
      "198       0.696      -1.534       0.730      -1.644       0.940      -1.348\n",
      "199       0.706      -1.534       0.488      -1.644      -0.699      -0.263\n",
      "200      -0.320      -1.534      -0.225      -1.644      -0.103       1.750\n",
      "201       0.706      -1.534       0.488      -1.644      -1.781       1.786\n",
      "202       0.008       1.724      -0.223       1.399      -0.712       0.882\n",
      "203       0.696      -1.534       0.730      -1.644       1.761      -1.402\n",
      "204      -0.320      -1.534      -0.225      -1.644      -0.648       2.299\n",
      "205       0.696      -1.534       0.730      -1.644       1.116      -1.711\n",
      "206       0.706      -1.534       0.488      -1.644      -0.103       1.750\n",
      "207      -0.320      -1.534      -0.225      -1.644      -0.648       1.319\n",
      "208       0.706      -1.534       0.488      -1.644      -0.570      -0.397\n",
      "209       0.706      -1.534       0.488      -1.644      -0.550       0.541\n",
      "210       0.706      -1.534       0.488      -1.644      -1.603       2.386\n",
      "211       0.706      -1.534       0.488      -1.644      -0.981      -1.085\n",
      "212      -1.498       1.724      -1.430       1.399      -1.551      -1.176\n",
      "213       0.706      -1.534       0.488      -1.644      -1.213      -0.282\n",
      "214      -0.320      -1.534      -0.225      -1.644       1.938      -0.991\n",
      "215       0.706      -1.534       0.488      -1.644      -1.765       0.717\n",
      "216       1.785      -1.534       1.908      -1.644       0.641      -1.338\n",
      "217       0.706      -1.534       0.488      -1.644      -0.800      -0.272\n",
      "218       0.706      -1.534       0.488      -1.644      -1.460      -0.641\n",
      "219       0.706      -1.534       0.488      -1.644      -0.155       0.509\n",
      "220       0.706      -1.534       0.488      -1.644      -0.993       0.554\n",
      "221       0.008       1.724      -0.223       1.399       0.724      -1.561\n",
      "222       0.706      -1.534       0.488      -1.644      -1.802       1.125\n",
      "223       0.706      -1.534       0.488      -1.644      -0.424       0.973\n",
      "224       0.696      -1.534       0.730      -1.644       0.308      -1.479\n",
      "225       0.706      -1.534       0.488      -1.644      -1.129      -0.500\n",
      "226      -0.320      -1.534      -0.225      -1.644      -1.527       0.203\n",
      "227       0.008       1.724      -0.223       1.399      -1.322       0.631\n",
      "228       0.706      -1.534       0.488      -1.644      -1.055      -0.594\n",
      "229       0.706      -1.534       0.488      -1.644      -1.603       1.799\n",
      "230      -0.224      -1.534       0.088      -1.644       1.841      -1.847\n",
      "231       0.706      -1.534       0.488      -1.644      -0.171       0.221\n",
      "232       0.696      -1.534       0.730      -1.644       0.927      -1.603\n",
      "233       0.706      -1.534       0.488      -1.644      -0.410       1.079\n",
      "234       0.706      -1.534       0.488      -1.644      -0.359       0.558\n",
      "235       0.696      -1.534       0.730      -1.644       0.676      -1.561\n",
      "236       0.706      -1.534       0.488      -1.644       0.007       1.722\n",
      "237       0.706      -1.534       0.488      -1.644      -0.390       0.534\n",
      "238       0.008       1.724      -0.223       1.399      -0.391       0.788\n",
      "239       0.706      -1.534       0.488      -1.644      -0.251      -0.024\n",
      "240       0.706      -1.534       0.488      -1.644      -1.838       0.344\n",
      "241       0.706      -1.534       0.488      -1.644      -0.225      -0.320\n",
      "242       0.706      -1.534       0.488      -1.644      -0.853      -0.833\n",
      "243       0.706      -1.534       0.488      -1.644      -0.877       1.058\n",
      "244       0.696      -1.534       0.730      -1.644       2.435      -1.237\n",
      "245       0.706      -1.534       0.488      -1.644      -0.579       0.579\n",
      "246       0.706      -1.534       0.488      -1.644      -0.814       1.228\n",
      "247       0.696      -1.534       0.730      -1.644       0.444      -1.820\n",
      "248       0.706      -1.534       0.488      -1.644      -0.360       1.392\n",
      "249       0.706      -1.534       0.488      -1.644      -0.528       0.207\n",
      "250       0.706      -1.534       0.488      -1.644      -0.545       0.179\n",
      "251       0.706      -1.534       0.488      -1.644      -0.055       1.505\n",
      "252       0.706      -1.534       0.488      -1.644      -1.515       0.797\n",
      "253       1.785      -1.534       1.908      -1.644       1.938      -0.991\n",
      "254       0.706      -1.534       0.488      -1.644      -0.914       0.530\n",
      "255       1.785      -1.534       1.908      -1.644       0.940      -1.348\n",
      "256       0.706      -1.534       0.488      -1.644      -0.703       1.726\n",
      "257       1.785      -1.534       1.908      -1.644       0.191      -1.444\n",
      "258       1.785      -1.534       1.908      -1.644       0.099      -1.452\n",
      "259       0.706      -1.534       0.488      -1.644      -0.209       0.066\n",
      "260       1.785      -1.534       1.908      -1.644       1.449      -1.157\n",
      "261       0.706      -1.534       0.488      -1.644      -1.780      -0.700\n",
      "262       0.706      -1.534       0.488      -1.644      -0.178       0.088\n",
      "263       0.008       1.724      -0.223       1.399       0.272      -1.681\n",
      "264       0.008       1.724      -0.223       1.399      -1.219       0.535\n",
      "\n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     34 PARAMETER v_sv  1-dim Matrix\n",
      "\n",
      "1    -3.390,    2   -12.683,    3   -23.339,    4   -10.827,    5   -23.339,    6   -23.339,    7   -23.339,    8   -11.608,    9   -23.339,    10   -1.138,    11  -13.102,    12  -23.339,    13   -1.347,    14  -23.339,    15  -23.339,    16   -0.265,    17  -23.339,    18  -23.339,    19  -14.132,    20  -23.339,    21  -23.339,    22  -23.339,    23   -7.533,    24   -3.562,    25  -15.532,    26  -13.168,    27   -0.503,    28   -0.788,    29  -13.753,    30  -23.339,    31  -23.339,    32   -0.216,    33  -23.339,    34   -4.485,    35  -23.339,    36   -8.254,    37  -22.872,    38   -3.651,    39  -23.339,    40  -23.339,    41    2.800,    42    2.800,    43    2.800,    44    2.800,    45    2.800,    46    2.800,    47    2.800,    48    2.800,    49    2.800,    50    2.800,    51    2.800,    52    2.800,    53    2.800,    54    2.800,    55    2.800,    56    2.800,    57    2.800,    58    2.800,    59    2.800,    60    2.763,    61    2.800,    62    2.800,    63    2.800,    64    2.800,    65    2.800,    66    2.800,    67    2.800,    68    2.800,    69    2.800,    70    2.800,    71    2.800,    72    2.800,    73    2.800,    74    2.800,    75    2.800,    76    2.800,    77    2.800,    78    2.800,    79    1.160,    80    2.800,    81    2.800,    82    1.553,    83    2.800,    84    2.800,    85    2.800,    86    2.800,    87    0.284,    88    2.800,    89    2.800,    90    2.800,    91    2.800,    92    2.800,    93    2.800,    94    2.800,    95    2.800,    96    2.800,    97    2.800,    98    2.800,    99    2.800,    100   1.822,    101   2.800,    102   2.800,    103   2.800,    104   2.800,    105   2.800,    106   2.800,    107   2.800,    108   2.800,    109   2.800,    110   2.800,    111   2.800,    112   2.800,    113   2.800,    114   2.800,    115   2.800,    116   1.168,    117   2.800,    118   2.800,    119   2.800,    120   2.800,    121   2.800,    122   2.301,    123   2.800,    124   2.800,    125   2.800,    126   2.800,    127   2.800\n",
      "128   2.800,    129   2.800,    130   2.800,    131   2.800,    132   2.800,    133   2.800,    134   2.800,    135   2.800,    136   2.800,    137   2.800,    138   2.800,    139   2.800,    140   2.800,    141   2.800,    142   2.581,    143   2.800,    144   2.800,    145   2.800,    146   2.800,    147   2.800,    148   2.800,    149   2.800,    150   2.800,    151   2.800,    152   2.800,    153   2.800,    154   2.800,    155   2.800,    156   2.337,    157   2.800,    158   2.800,    159   2.800,    160   2.800,    161   2.421,    162   2.800,    163   2.800,    164   2.800,    165   0.411,    166   2.800,    167   2.800,    168   2.800,    169   2.800,    170   2.800,    171   2.800,    172   2.800,    173   2.800,    174   2.800,    175   2.800,    176   2.800,    177   2.800,    178   2.800,    179   2.800,    180   2.800,    181   2.800,    182   2.800,    183   2.800,    184   2.800,    185   2.800,    186   2.800,    187   2.800,    188   2.800,    189   2.800,    190   2.800,    191   2.800,    192   1.169,    193   2.800,    194   2.800,    195   2.800,    196   2.800,    197   2.800,    198   2.800,    199   2.800,    200   2.800,    201   2.800,    202   2.800,    203   2.800,    204   2.800,    205   2.800,    206   2.800,    207   2.800,    208   2.800,    209   2.800,    210   2.800,    211   2.800,    212   1.249,    213   2.800,    214   2.800,    215   2.800,    216   2.800,    217   2.800,    218   2.800,    219   2.800,    220   2.800,    221   1.972,    222   2.800,    223   2.800,    224   2.800,    225   2.800,    226   2.800,    227   2.800,    228   2.800,    229   2.800,    230   2.800,    231   0.802,    232   2.800,    233   2.800,    234   2.800,    235   2.800,    236   2.800,    237   2.800,    238   2.800,    239   2.800,    240   2.800,    241   2.800,    242   2.800,    243   2.800,    244   2.800,    245   2.800,    246   2.800,    247   2.800,    248   2.800,    249   2.800,    250   2.800,    251   2.800,    252   2.800,    253   2.800,    254   2.800\n",
      "255   2.615,    256   2.800,    257   2.800,    258   2.800,    259   2.800,    260   2.800,    261   2.800,    262   0.448,    263   2.800,    264   2.419\n",
      "\n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n",
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     40 PARAMETER input_offset  1-dim Matrix\n",
      "\n",
      "1 2.582,    2 2.602,    3 2.787,    4 2.906,    5 0.333,    6 0.333\n",
      "\n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n",
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     46 PARAMETER input_gain  1-dim Matrix\n",
      "\n",
      "1 1.303,    2 1.277,    3 1.055,    4 0.828,    5 5.555,    6 5.574\n",
      "\n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n",
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     53 PARAMETER bias                 =        3.884  \n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb15.gdx'\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n",
      "E x e c u t i o n\n",
      "\n",
      "\n",
      "----     60 PARAMETER d                    =        4.000  \n",
      "\n",
      "\n",
      "Warning: [WinError 32] The process cannot access the file because it is being used by another process: '_gams_py_gdb30.gdx'\n"
     ]
    }
   ],
   "source": [
    "%gams_push x_sv\n",
    "%gams display x_sv;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k\n",
    "\n",
    "\n",
    "%gams_push v_sv\n",
    "%gams display v_sv;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k\n",
    "\n",
    "%gams_push input_offset\n",
    "%gams display input_offset;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k\n",
    "\n",
    "%gams_push input_gain\n",
    "%gams display input_gain;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k\n",
    "\n",
    "\n",
    "%gams_push bias\n",
    "%gams display bias;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k\n",
    "\n",
    "%gams_push d\n",
    "%gams display d;\n",
    "%gams_lst -e\n",
    "%gams_cleanup -k"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
